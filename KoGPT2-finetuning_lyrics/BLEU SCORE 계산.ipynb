{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 : https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REF 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 아이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 129\n",
      "1 100\n",
      "2 100\n",
      "3 80\n",
      "4 104\n",
      "5 76\n",
      "6 90\n",
      "7 96\n",
      "8 92\n",
      "9 91\n",
      "10 111\n",
      "11 92\n",
      "12 78\n",
      "13 120\n",
      "14 103\n",
      "15 95\n",
      "16 100\n",
      "17 73\n",
      "18 114\n",
      "19 80\n",
      "20 72\n",
      "21 102\n",
      "22 76\n",
      "23 105\n",
      "24 108\n",
      "25 62\n",
      "26 81\n",
      "27 76\n",
      "28 107\n",
      "29 68\n",
      "30 76\n",
      "31 65\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/iu_start.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "all_words_iu = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['iu_lyrics'][i]\n",
    "    words = kkma.morphs(text)\n",
    "    all_words_iu.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(all_words_iu)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 선우정아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 94\n",
      "1 112\n",
      "2 131\n",
      "3 104\n",
      "4 126\n",
      "5 101\n",
      "6 104\n",
      "7 96\n",
      "8 134\n",
      "9 91\n",
      "10 118\n",
      "11 120\n",
      "12 115\n",
      "13 98\n",
      "14 89\n",
      "15 109\n",
      "16 99\n",
      "17 99\n",
      "18 157\n",
      "19 92\n",
      "20 111\n",
      "21 96\n",
      "22 129\n",
      "23 67\n",
      "24 95\n",
      "25 103\n",
      "26 98\n",
      "27 100\n",
      "28 107\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/s_start.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "all_words_s = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['s_lyrics'][i]\n",
    "    words = kkma.morphs(text)\n",
    "    all_words_s.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(all_words_s)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 몬스타엑스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142\n",
      "1 82\n",
      "2 97\n",
      "3 82\n",
      "4 113\n",
      "5 87\n",
      "6 96\n",
      "7 102\n",
      "8 115\n",
      "9 112\n",
      "10 95\n",
      "11 102\n",
      "12 79\n",
      "13 138\n",
      "14 120\n",
      "15 93\n",
      "16 82\n",
      "17 111\n",
      "18 111\n",
      "19 99\n",
      "20 109\n",
      "21 94\n",
      "22 124\n",
      "23 84\n",
      "24 107\n",
      "25 104\n",
      "26 103\n",
      "27 106\n",
      "28 109\n",
      "29 122\n",
      "30 87\n",
      "31 106\n",
      "32 112\n",
      "33 115\n",
      "34 106\n",
      "35 64\n",
      "36 122\n",
      "37 117\n",
      "38 104\n",
      "39 117\n",
      "40 169\n",
      "41 83\n",
      "42 120\n",
      "43 124\n",
      "44 131\n",
      "45 79\n",
      "46 136\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/mx_start.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "all_words_mx = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['mx_lyrics'][i]\n",
    "    words = kkma.morphs(text)\n",
    "    all_words_mx.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(all_words_mx)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAN 생성한 가사 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 아이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 167\n",
      "1 166\n",
      "2 161\n",
      "3 179\n",
      "4 153\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/생성가사.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "iu_gen = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['iu'][i]\n",
    "    words = kkma.morphs(text)\n",
    "    iu_gen.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(iu_gen)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['시간', '의', '존재', '도', '시간', '과', '함께', '소멸', '되', '고', '파', '아', '기억', '은', '사라지', '어', '버리', '어요', '잠', '시', '만', '눈', '을', '떼', '면', '세상', '모든', '것', '이', '멈추', '죠', '아프', '지', '않', '은', '곳', '으로', '다시', '돌아가', 'ㅂ니다', '그', '대', '눈', '을', '뜨', '면', '그대', '맘', '도', '알아주', 'ㄹ', '거', '이', '에요', '그렇게', '하루', '하루', '를', '살', '아요', '시간', '이', '지', '나도', '끝나', '지', '않', '은', '그대', '사랑', '도', '잊히', '어', '버리', '었', '죠', '이제', '그만', '잊', '어요', '잊', '지', '않', '을게요', '아무', '에게', '도', '말하', '지', '않', '을게요', '시간', '이', '지', '나도', '끝나', '지', '않', '은', '그대', '사랑', '도', '잊히', '어', '버리', '었', '죠', '이제', '그만', '잊', '어요', '잊', '을게요', '아무', '에게', '도', '말하', '지', '않', '을게요', '아주', '오래전', '사랑', '하', 'ㄴ', '약속', '도', '잊히', '어', '버리', '었', '죠', '사랑', '에', '눈', '뜨', 'ㄴ', '지금', '도', '변하', 'ㅁ', '이', '없', '죠', '그러', 'ㄴ', '그대', '사랑', '은', '낳', 'ㄹ', '추억', '하', '는', '것', '이', '죠', '기억', '에', '되살아나', '는', '그대', '사랑', '<', '/', 's', '>'], ['시간', '나', '면', '전화', '하', '어', '사랑', '하', 'ㄴ다면', '내', '게', '로', '오', 'ㄹ래요', '오', 'ㄹ래', '안', '오', 'ㄹ래요', '오', 'ㄹ래', '안', '오', 'ㄹ래요', '오', 'ㄹ래', '대답', '없', '는', '그대', '마음', '알', '아요', '나', '의', '맘', '이', '나', '를', '얼마나', '사랑', '하', '는지', '묻', '지', '말', '아요', '하루', '에', '도', '몇', '번', '씩', '내', '맘', '을', '아프', '게', '하', '니', '사랑', '하', 'ㄴ다면', '내', '게', '로', '오', 'ㄹ래요', '오', 'ㄹ래', '안', '오', 'ㄹ래요', '오', 'ㄹ래', '대답', '없', '는', '그대', '마음', '알', '아요', '나', '의', '맘', '이', '나', '를', '얼마나', '사랑', '하', '는지', '묻', '지', '말', '아요', '하루', '에', '도', '몇', '번', '씩', '내', '맘', '을', '아프', '게', '하', '니', '사랑', '하', 'ㄴ다면', '내', '게', '로', '오', 'ㄹ래요', '오', 'ㄹ래', '안', '오', 'ㄹ래요', '오', 'ㄹ래', '대답', '없', '는', '그대', '마음', '알', '아요', '나', '의', '맘', '이', '나', '를', '얼마나', '사랑', '하', '는지', '묻', '지', '말', '아요', '하루', '에', '도', '몇', '번', '씩', '내', '맘', '을', '아프', '게', '하', '니', '사랑', '하', 'ㄴ다면', '내', '게', '로', '오', '세요'], ['시간표', '도', '모르', '아서', '뭐하', '어', '숙제', '하', 'ㄹ까', '보', '아', '왜', '자꾸', '눈물', '만', '나', '와요', '별일', '아니', '지', '궁금하', '어서', '왜', '자꾸', '웃', '어요', '얘기', '나누', '다', '보', '면', '시간', '안', '갈', '는', '것', '이', '없', '잖아요', '왜', '자꾸', '울', '어요', '별일', '아니', '지', '궁금하', '어서', '왜', '자꾸', '웃', '어요', '얘기', '나누', '다', '보', '면', '시간', '안', '갈', '는', '것', '이', '없', '잖아', '왜', '자꾸', '울', '어요', '별일', '아니', '지', '궁금하', '어서', '왜', '자꾸', '웃', '어요', '얘기', '나누', '다', '보', '면', '시간', '안', '갈', '는', '것', '이', '없', '잖아', '왜', '자꾸', '울', '어요', '별일', '아니', '지', '궁금하', '어서', '왜', '자꾸', '울', '어요', '얘기', '나누', '다', '보', '면', '시간', '안', '갈', '는', '것', '이', '없', '잖아', '왜', '자꾸', '자꾸', '울', '어요', '별일', '아니', '지', '궁금하', '어서', '왜', '자꾸', '울', '어요', '그', '대', '마음', '을', '알', '잖아요', '나', '를', '알', '잖아', '나', '를', '알', '잖아', '나', '를', '알', '잖아', '나', '를', '알', '잖아', '나', '를', '알', 'ㄹ', '<', '/', 's', '>'], ['시간', '이', '라는', '이름', '의', '시작', '이', '구', '나아', '득', '하', 'ㄴ', '곳', ',', '시간', '보다', '멀', 'ㄴ', '곳', '나의', '눈', '을', '피하', '어', '떨', '나', '보', '는', '시간', '생각', '하', '어', '보', '아', '그리하', '여', '너', '를', '알', '게', '되', '어서', '나', '에게', '시간', '이란', '무엇', '이', 'ㄴ가', '오늘', '따라', '너무나', '도', '외로워너에게', '만', '갈', 'ㄹ', '수', '없', '는', '곳', '외', '롭', 'ㄴ', '시간', '처럼', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '란', '시간', '을', '알', '아도', '너무나', '도', '시간', '이', '흐르', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '만', '싶', '어', '시간', '이', '가', '면', '멀', '어', '지', '고', '<', '/', 's', '>'], ['시간', '이', '란', '말', '을', '떠올리', 'ㄴ', '적이', '없었', '죠', '그런데', '지나', '고', '나', '면', '또', '다시', '그', '말', '이', '떠오르', '았', '죠', '아무', '도', '없', '는', '밤하늘', '에', '홀로', '서', '어', '있', '는', '기분', '이', '었', '죠', '별', '을', '찾', '아', '헤매', 'ㄴ', '시간', '들', '당신', '의', '눈', '에', '는', '아직', '도', '이', '밤', '그대', '가', '오', 'ㄴ', '거', '이', '에요', '내', '가', '그대', '를', '만나', '었', '었', '죠', '내', '마음', '은', '그대', '가', '내', '맘', '속', '에', '있', '습니다', '언제나', '그대', '를', '만나', '면', '두근거리', '는', '마음', '에', '흔들', '려요', '언제나', '그대', '를', '만나', '면', '두근거리', '는', '마음', '에', '흔들', '려요', '어떻', 'ㄴ', '말로', '도', '그', '대가', '날', '안', '았', '더라면', '좋', '았', '겠', '죠', '날', '안', '았', '더라면', '좋', '았', '겠', '네요', '내', '마음', '이', '그대', '를', '만나', '면', '두근거리', '는', '마음', '에', '흔들', '려요', '언제나', '그대', '를', '만나', '면', '두근거리', '는', '마음', '에', '흔들리', '어', '<', '/', 's', '>']]\n"
     ]
    }
   ],
   "source": [
    "print(iu_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 선우정아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 208\n",
      "1 122\n",
      "2 186\n",
      "3 219\n",
      "4 293\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/생성가사.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "s_gen = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['s'][i] # 선우정아\n",
    "    words = kkma.morphs(text)\n",
    "    s_gen.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(s_gen)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['시간', '은', '말하', '어', '주', '는데', '너', '는', '언제쯤', '울', '어', '울', '어', '생각', '은', '다', '하', '고', '그러나', '시간', '이', '란', '말', '은', '너', '에게', '만', '많', '은', '것', '을', '말하', '어', '주', '어', '눈감', '아', '주기', '만', '하', '잖아', '어디', '서', '누구', '와', '드', 'ㄴ', '대화', '를', '하', '어야', '알', '잖아', '왜', '낳', 'ㄴ', '숨', '이', '막히', '고', '답답', '하', '어', '참으', 'ㄹ', '수', '없', '는', '밤', '이', '되', '어', '자', '면', '몇', '시간', '만', '에', '지치', '어', '버리', 'ㄹ', '때', '오직', '너와', '나', '아', '이렇', '게', 'You', '곁', '에', '있', '어', '주', '어', 'You', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '나', '는', '언제나', '그리하', '였', '어', 'You', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '사실', '낳', 'ㄴ', 'Youdo', 'look', 'awayYou', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '나', '는', '언제나', '그리하', '였', '어', 'You', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '사실', '낳', 'ㄴ', 'Youdo', 'look', 'awayYou', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '사실', '낳', 'ㄴ', 'Youdo', 'look', 'awayYou', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '나', '는', '언제나', '그리하', '였', '어', 'You', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '사실', '낳', 'ㄴ', 'Youdo', 'look', 'awayYou', \"'\", 're', 'my', 'brainshut', 'youdo', 'look', 'away', '나', '는', '언제나', '그리하', '였', '어', 'You', \"'\", 're', 'my'], ['시간', '의', '신비', '에', '잠기', 'ㄴ', '밤하늘', '에', '내던지', '어', '지네', '뜨겁', '게', '숨쉬', '는', '별', '소원', '하', '었', '지만', '아름답', '게', '빛나', '지', '못하', '었', '네', '어두움', '은', '날', '째이', '어', '오', '고', '태양', '도', '날', '떨', '나', '쓰러지', '어', '갈', '는', '내', '몸', '조각', '들', '을', '영원', '하', 'ㄴ', '곳', '에', '안녕', '여기', '는', '어', '어', '디', 'ㄴ지', '숨', '이', '막히', '어', '오', '아', '강렬', '하', 'ㄴ', '혼란', '이', '나', '를', '덮치', '어', '오', '아', '눈뜨', '면', '새롭', 'ㄴ', '하늘', '이', '지만', '다시', '또다시', '빛나', '지', '못하', '었', '네', '어두움', '은', '날', '째이', '어', '오', '고', '태양', '도', '날', '떨', '나', '쓰러지', '어', '갈', '는', '내', '몸', '조각', '들', '을', '영원', '하', 'ㄴ', '곳', '에', '안녕', '이젠', '모두', '영원히', '안'], ['시간', '갈', '는', '줄', '모르', '게', '해', '요', '어둡', '어', '어둡', '어', '어둡', '어', '밤', '사라지', '지', '않', '게', '숨', '이', '막히', 'ㄹ', '것', '만', '같', '은', '길', 'ㄴ', '방', '안', '소파', '에', '기대어', '지루', '하', 'ㄴ', '시간', '을', '보내', '었', '더', 'ㄴ', '건지', 'ㄴ', '모르', '았', '는데', '알', '고', '있', '었', '죠', '나', '는', '아무것', '도', '모르', '고', '방', '에', '만', '있', '었', '죠', 'You', \"'\", 're', 'so', 'beautiful', 'to', 'meYou', \"'\", 're', 'so', 'beautiful', 'to', 'meDon', \"'\", 't', 'care', '어리', 'ㄴ', '내가', '무엇', '을', '좋아하', '는지', '무슨', '생각', '하', '는지', '몰르', '았', '는데', 'You', \"'\", 're', 'so', 'beautiful', 'to', 'meSomething', 'I', \"'\", 'm', 'the', 'other', 'way', 'to', 'meYou', \"'\", 're', 'so', 'beautiful', 'to', 'meDon', \"'\", 't', 'care', '어리', 'ㄴ', '내가', '무엇', '을', '좋아하', '는지', '몰르', '았', '는데', '알', '고', '있', '었', '죠', '나', '는', '아무것', '도', '모르', '고', '방', '에', '만', '있', '었', '죠', 'You', \"'\", 're', 'so', 'beautiful', 'to', 'meSomething', 'I', \"'\", 'm', 'the', 'other', 'way', 'to', 'meDon', \"'\", 't', 'care', '어리', 'ㄴ', '내가', '무엇', '을', '좋아하', '는지', '몰르', '았', '는데', '알', '고', '있', '었', '죠', '나', '는', '아무것', '도', '모르', '았', '죠'], ['시간', '갈', '는', '줄', '모르', '겠', '어', '어자', '떠나', '아야', '하', '어', '길', '을', '나', '서', '야', '해어', '디', 'ㄹ', '향', '하', '어', '가', '는지', '몰르', '아도', '어디', '서', '부터', '나', '는', '잘못되', '었', '을까', '모든', '건', '내', '맘', '같', '을', '수', '없', '잖아', '다', '지우', '어야', '하', '어', '살', '아', '내야', '만', '하', '어', '모두', '다', '제', '갈', 'ㄹ', '길', '로', '기다리', '어', '주', '어', '이', '노래', 'ㄹ', '다', '만들', 'ㄹ', '때', '까지', '마지막', '코드', '가', '다', '끝나', 'ㄹ', '때', '까지', '내', '힘껏', '기타', '다운', '스트로크', '세상', '이', '다', '변하', 'ㄴ다', '하', '어도', '내', '목소리', '몇', '번', '씩', '갈라지', '어도', '넣', 'ㄹ', '향', '하', 'ㄴ', '나', '의', '갈증', '은', '날', '떠나', '지', '못하', 'ㄹ', '그', '어야', '다', '지우', '어야', '하', '어', '살', '아', '내야', '만', '하', '어', '길', '을', '나', '서', '야', '하', '어', '나', '를', '위하', '어', '노래', '하', '었', '잖아', '모든', '건', '다', '낳', 'ㄹ', '비웃', '고', '있', '는', '거', '야', '날', '위', '하', '어', '춤', '하', '었', '잖아', '모든', '것', '이', '다', '낳', 'ㄹ', '비웃', '고', '있', '는', '그', '어야', '다', '지우', '어야', '하', '어', '사라지', '고', '싶', '어', '다', '지우', '어야', '하', '어', '사라지', '고', '싶', '어', '다', '지우', '어야', '하', '어', '사라지', '고', '싶', '어', '다', '지우', '어야', '하', '어', '사라지', '고', '싶', '어', '다', '지우', '어야', '하', '어', '사라지', '고', '싶', '어'], ['시간', '갈', '는', '줄', '모르', '고', '참', '고단', '하', 'ㄴ', '하루', '이', '었', '습니다', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '인생', '표가', '제', '다시', '그리', '어', '지', 'ㅂ니다', '내', '인생', '표', '가', '또', '그리', '어', '지네', '꿈', '을', '꾸', '듯', '곱', '게', '만', '그리', '어', '오', '더', 'ㄴ', '나의', '미래', '인생', '표', '가']]\n"
     ]
    }
   ],
   "source": [
    "print(s_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 몬스타엑스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 244\n",
      "1 283\n",
      "2 291\n",
      "3 275\n",
      "4 227\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/생성가사.xlsx')\n",
    "num_df =len(df) \n",
    "\n",
    "## 형태소 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# 형태소 분석 모델 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "# 전체 단어 넣을 리스트\n",
    "mx_gen = []\n",
    "\n",
    "\n",
    "for i in range(num_df):\n",
    "    text = df['mx'][i] # 몬스타엑스\n",
    "    words = kkma.morphs(text)\n",
    "    mx_gen.append(words)\n",
    "    print(i, len(words))\n",
    "\n",
    "print(len(mx_gen)) # ->  이게 REFERENCE가 되는 것\n",
    "# unique_words = list(set(all_words)) # 중복 제거한 형태소 개수 -> 이게 REFERENCE가 되는 것\n",
    "# print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['시간', '많', '아도', '체력', '을', '아끼', '어', '야죠', '우리', '사이', '는', '바쁘', '게', '흘러가', '죠', '아프', 'ㄴ', '기억', '들', '은', '저', '만이', '하', 'ㄹ', '수', '있', '는', '표현', '이', '아니', 'ㄹ', '런지', '요', '이제', '우리', '는', '함께', '웃', '어야', '겠어', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니', '언젠가', '다', '지우', 'ㄹ', '수', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘', '길', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '이', '야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이야', '이', '야', '얘기', '를', '하다', '보', '면', '전부', '지우', 'ㄹ', '수', '도', '있', '으니깐', '우리', 'ㄴ', '해피엔딩', '이', '야'], ['시간', '멈추', 'ㄴ', '듯', '정신없', '는', '하', '로', '이끌리', '듯', '속삭이', '더', 'ㄴ', '나', '를', '일으키', '어', '세운', '건', '니', '고린도', '전', '‘', '마시', '어', '지', '어', '라', '매직', '피버', '’', '또', '같', '은', '꿈', '을', '꾸', '며', '계속', '울리', '고', '있', '어', '눈부시', '게', '더', '크', '게', '커지', '어', '버리', 'ㄴ', '니', '마음', '도', '나', '를', '덮치', '어', '오', '아', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어', 'A', 'yo', 'ya', '감히', '누', '가', '널', '거부', '하', 'ㄹ', '수', '있', '어', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어', 'A', 'yo', 'ya', '감히', '누', '가', '널', '거부', '하', 'ㄹ', '수', '있', '어', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어', 'A', 'yo', 'yo', 'ya', '감히', '누', '가', '널', '거부', '하', 'ㄹ', '수', '있', '어', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어', 'A', 'yo', 'ya', '감히', '누', '가', '널', '거부', '하', 'ㄹ', '수', '있', '어', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어', 'A', 'yo', 'ya', '감히', '누', '가', '널', '거부', '하', 'ㄹ', '수', '있', '어', 'I', 'don', '’', 't', 'wanna', 'kiss', 'you', '너', '는', '깜짝', '놀라', 'ㄹ', '거', '야', '너', '로', '인하', '어', '시간', '마저', '멈추', '어', '버리', 'ㄴ', '듯하', '어'], ['시간', '마저', \"'\", '아쉽', '어', '아쉽', '었', '더', 'ㄴ', '날', '들', 'You', \"'\", 're', 'so', 'sick', 'ofday', '짧', '은', '꿈', '이', 'ㄴ', '줄', '로', '만', '알', '았', '는데', '알', '니', '덜', 'ㄴ', '사실', '널', '둘러쌓', 'ㄴ', '수많', '은', '수많', '은', '시선', '중', '내', '게만', '알', '려', '지', '었', '던', 'days', '길', '잃', '은', '기분', '이', '듣', '어', '나', '의', '모든', '것', '을', '내리', '어', '놓', '았', '어', '널', '다시', '붙잡', '고', '싶', '어', 'Cuz', 'I', \"'\", 'm', 'not', 'sick', 'ofday', '날', '당기', '는', '것', '은', '너', '의', '목소리', '야', '눈부시', '게', '아름답', 'ㄴ', '너', '의', '목소리', 'Where', 'you', 'at', '지금', '기다리', '고', '있', '는', '내', '얼굴', '을', '뒤로하', 'ㄴ', '채', 'Where', 'you', 'at', '지금', '어떻', '게', '날', '두고', '떨', '나', 'babyWhere', 'you', 'at', '지금', '어떻', '게', '날', '두고', '떨', '나', 'baby', '너', '의', '모든', '것', '을', '내리', '어', '놓', '았', '어', '날', '두고', '떨', '나', 'babyWhere', 'you', 'at', '지금', '어떻', '게', '날', '두고', '떨', '나', 'baby', '너', '의', '모든', '것', '을', '내리', '어', '놓', '았', '어', '날', '두고', '떨', '나', 'babyWhere', 'you', 'at', '지금', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '내', '마지막', '남', '은', '꿈', '이', 'ㄴ', '것', '같이', '네', '가', '아', '내', '곁', '에', '있', '다면', '행복', '하', '었', '어', 'Where', 'you', 'at', '지금', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '아', '내', '곁', '을', '떠나', 'ㄹ', '때', '에는', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '아', '내', '곁', '을', '떠나', 'ㄹ', '때', '에는', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '아', '내', '곁', '을', '떠나', 'ㄹ', '때', '에는', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '아', '내', '곁', '을', '떠나', 'ㄹ', '때', '에는', '어떻', '게', '날', '두고', '떨', '나', 'baby', '네', '가', '아', '내', '곁', '을', '떠나', 'ㄹ'], ['시간', '이', '멈추', 'ㄴ', '듯', '멈추', '어', '있', '어', '니', \"'\", '귓가', '에', '울리', '어', '퍼지', '더', 'ㄴ', '널', '위', '하', 'ㄴ', \"'\", '귓속말', \"'\", '너', '는', '마치', '엄마', '처럼', 'Huh', '기다리', '어', '주', '어', 'Huh', '알', '잖아', '지금', '너', '는', '내', '게', '모든', '것', '을', '던지', '어', '넣', 'ㄴ', '날', '무너뜨리', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가', '어디', '에', '있', '어', '나', '가', '여기', '있', '다는', '것', '을', '너', '는', '이미', '알', '고', '있', '어', 'Woo', 'bring', 'me', 'uh', '시간', '이', '멈추', '어', '있', '어', '니', '가'], ['시간', '참', '빠르', '게', '지나가', 'ㄴ다', '느', '어도', '그러', 'ㄹ', '거', '야', '나', '는', '너', '의', '모든', '것', '을', '다', '가지', 'ㄴ', '것', '같', '아', '니', '가', '나', '를', '넘보', 'ㄹ', '수', '없', '게', '니', '가', '내', '여자', '이', '라는', '거널', '갖', '고', '싶', '어', 'I', 'can', 'fly', 'you', 'fly', 'you', 'let', 'youand', 'your', 'eyes', 'you', 'make', 'me', 'your', 'eyes', 'you', 'make', 'me', 'your', 'eyes', '어둡', '었', '더', 'ㄴ', '날', '들', '에', '이제', '너', '를', '대입', '하', '어', '나', '를', '그리', '어', '주', '어', '내', '모든', '것', '을', '다', '내', '게', '주', '어', 'Every', 'day', 'every', 'nightI', 'can', 'fly', 'you', 'let', 'youand', 'your', 'eyes', 'you', 'make', 'me', 'your', 'eyes', '어둡', '었', '더', 'ㄴ', '날', '들', '에', '이제', '너', '를', '대입', '하', '어', '나', '를', '그리', '어', '주', '어', '내', '모든', '것', '을', '다', '주', '어', 'Every', 'day', 'every', 'nightI', 'can', 'fly', 'you', 'let', 'youand', 'your', 'eyes', 'you', 'make', 'me', 'your', 'eyes', '어둡', '었', '더', 'ㄴ', '날', '들', '에', '이제', '너', '를', '대입', '하', '어', '나', '를', '그리', '어', '주', '어', '내', '모든', '것', '을', '다', '주', '어', 'Every', 'day', 'every', 'nightI', 'can', 'fly', 'you', 'let', 'youand', 'your', 'eyes', 'you', 'make', 'me', 'your', 'eyes', '어둡', '었', '더', 'ㄴ', '날', '들', '에', '이제', '너', '를', '대입', '하', '어', '나', '를', '그리', '어', '주', '어', '내', '모든', '것', '을', '다', '주', '어', 'Every', 'day', 'every', 'nightI', 'can', 'fly', 'you', 'le']]\n"
     ]
    }
   ],
   "source": [
    "print(mx_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 아이유 데이터   CAN = 아이유 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.718563\n",
      "Cumulative 2-gram: 0.366319\n",
      "Cumulative 3-gram: 0.172650\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.530120\n",
      "Cumulative 2-gram: 0.240481\n",
      "Cumulative 3-gram: 0.072546\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.440994\n",
      "Cumulative 2-gram: 0.228841\n",
      "Cumulative 3-gram: 0.070930\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.541899\n",
      "Cumulative 2-gram: 0.326425\n",
      "Cumulative 3-gram: 0.171904\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.673203\n",
      "Cumulative 2-gram: 0.415608\n",
      "Cumulative 3-gram: 0.415608\n",
      "\n",
      "\n",
      "avg 1-gram: 0.580956\n",
      "avg 2-gram: 0.315535\n",
      "avg 3-gram: 0.138267\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "iu_b1_1=sentence_bleu(all_words_iu, iu_gen[0], weights=(1, 0, 0, 0))\n",
    "iu_b1_2=sentence_bleu(all_words_iu, iu_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "iu_b1_3=sentence_bleu(all_words_iu, iu_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_b1_1)\n",
    "print('Cumulative 2-gram: %f' % iu_b1_2)\n",
    "print('Cumulative 3-gram: %f' % iu_b1_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_b2_1=sentence_bleu(all_words_iu, iu_gen[1], weights=(1, 0, 0, 0))\n",
    "iu_b2_2=sentence_bleu(all_words_iu, iu_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "iu_b2_3=sentence_bleu(all_words_iu, iu_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_b2_1)\n",
    "print('Cumulative 2-gram: %f' % iu_b2_2)\n",
    "print('Cumulative 3-gram: %f' % iu_b2_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_b3_1=sentence_bleu(all_words_iu, iu_gen[2], weights=(1, 0, 0, 0))\n",
    "iu_b3_2=sentence_bleu(all_words_iu, iu_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "iu_b3_3=sentence_bleu(all_words_iu, iu_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_b3_1)\n",
    "print('Cumulative 2-gram: %f' % iu_b3_2)\n",
    "print('Cumulative 3-gram: %f' % iu_b3_3)\n",
    "print('\\n')\n",
    "      \n",
    "iu_b4_1=sentence_bleu(all_words_iu, iu_gen[3], weights=(1, 0, 0, 0))\n",
    "iu_b4_2=sentence_bleu(all_words_iu, iu_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "iu_b4_3=sentence_bleu(all_words_iu, iu_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_b4_1)\n",
    "print('Cumulative 2-gram: %f' % iu_b4_2)\n",
    "print('Cumulative 3-gram: %f' % iu_b4_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_b5_1=sentence_bleu(all_words_iu, iu_gen[4], weights=(1, 0, 0, 0))\n",
    "iu_b5_2=sentence_bleu(all_words_iu, iu_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "iu_b5_3=sentence_bleu(all_words_iu, iu_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_b5_1)\n",
    "print('Cumulative 2-gram: %f' % iu_b5_2)\n",
    "print('Cumulative 3-gram: %f' % iu_b5_2)\n",
    "print('\\n')\n",
    "avg1=(iu_b1_1+iu_b2_1+iu_b3_1+iu_b4_1+iu_b5_1)/5\n",
    "avg2=(iu_b1_2+iu_b2_2+iu_b3_2+iu_b4_2+iu_b5_2)/5\n",
    "avg3=(iu_b1_3+iu_b2_3+iu_b3_3+iu_b4_3+iu_b5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 아이유 데이터 # CAN = 선우정아 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.495192\n",
      "Cumulative 2-gram: 0.324435\n",
      "Cumulative 3-gram: 0.155830\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.745902\n",
      "Cumulative 2-gram: 0.351126\n",
      "Cumulative 3-gram: 0.175605\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.532258\n",
      "Cumulative 2-gram: 0.317329\n",
      "Cumulative 3-gram: 0.166581\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.552511\n",
      "Cumulative 2-gram: 0.345137\n",
      "Cumulative 3-gram: 0.173357\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.279863\n",
      "Cumulative 2-gram: 0.107244\n",
      "Cumulative 3-gram: 0.107244\n",
      "\n",
      "\n",
      "avg 1-gram: 0.521145\n",
      "avg 2-gram: 0.289054\n",
      "avg 3-gram: 0.141321\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "iu_sb1_1=sentence_bleu(all_words_iu, s_gen[0], weights=(1, 0, 0, 0))\n",
    "iu_sb1_2=sentence_bleu(all_words_iu, s_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "iu_sb1_3=sentence_bleu(all_words_iu, s_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_sb1_1)\n",
    "print('Cumulative 2-gram: %f' % iu_sb1_2)\n",
    "print('Cumulative 3-gram: %f' % iu_sb1_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_sb2_1=sentence_bleu(all_words_iu, s_gen[1], weights=(1, 0, 0, 0))\n",
    "iu_sb2_2=sentence_bleu(all_words_iu, s_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "iu_sb2_3=sentence_bleu(all_words_iu, s_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_sb2_1)\n",
    "print('Cumulative 2-gram: %f' % iu_sb2_2)\n",
    "print('Cumulative 3-gram: %f' % iu_sb2_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_sb3_1=sentence_bleu(all_words_iu, s_gen[2], weights=(1, 0, 0, 0))\n",
    "iu_sb3_2=sentence_bleu(all_words_iu, s_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "iu_sb3_3=sentence_bleu(all_words_iu, s_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_sb3_1)\n",
    "print('Cumulative 2-gram: %f' % iu_sb3_2)\n",
    "print('Cumulative 3-gram: %f' % iu_sb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "iu_sb4_1=sentence_bleu(all_words_iu, s_gen[3], weights=(1, 0, 0, 0))\n",
    "iu_sb4_2=sentence_bleu(all_words_iu, s_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "iu_sb4_3=sentence_bleu(all_words_iu, s_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_sb4_1)\n",
    "print('Cumulative 2-gram: %f' % iu_sb4_2)\n",
    "print('Cumulative 3-gram: %f' % iu_sb4_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_sb5_1=sentence_bleu(all_words_iu, s_gen[4], weights=(1, 0, 0, 0))\n",
    "iu_sb5_2=sentence_bleu(all_words_iu, s_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "iu_sb5_3=sentence_bleu(all_words_iu, s_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_sb5_1)\n",
    "print('Cumulative 2-gram: %f' % iu_sb5_2)\n",
    "print('Cumulative 3-gram: %f' % iu_sb5_2)\n",
    "print('\\n')\n",
    "avg1=(iu_sb1_1+iu_sb2_1+iu_sb3_1+iu_sb4_1+iu_sb5_1)/5\n",
    "avg2=(iu_sb1_2+iu_sb2_2+iu_sb3_2+iu_sb4_2+iu_sb5_2)/5\n",
    "avg3=(iu_sb1_3+iu_sb2_3+iu_sb3_3+iu_sb4_3+iu_sb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 아이유 데이터 # CAN = 몬엑 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.282787\n",
      "Cumulative 2-gram: 0.118173\n",
      "Cumulative 3-gram: 0.050181\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.349823\n",
      "Cumulative 2-gram: 0.199239\n",
      "Cumulative 3-gram: 0.110768\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.474227\n",
      "Cumulative 2-gram: 0.245977\n",
      "Cumulative 3-gram: 0.126123\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.330909\n",
      "Cumulative 2-gram: 0.183890\n",
      "Cumulative 3-gram: 0.106066\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.458150\n",
      "Cumulative 2-gram: 0.273874\n",
      "Cumulative 3-gram: 0.273874\n",
      "\n",
      "\n",
      "avg 1-gram: 0.379179\n",
      "avg 2-gram: 0.204231\n",
      "avg 3-gram: 0.110051\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "iu_mb1_1=sentence_bleu(all_words_iu, mx_gen[0], weights=(1, 0, 0, 0))\n",
    "iu_mb1_2=sentence_bleu(all_words_iu, mx_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "iu_mb1_3=sentence_bleu(all_words_iu, mx_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_mb1_1)\n",
    "print('Cumulative 2-gram: %f' % iu_mb1_2)\n",
    "print('Cumulative 3-gram: %f' % iu_mb1_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_mb2_1=sentence_bleu(all_words_iu, mx_gen[1], weights=(1, 0, 0, 0))\n",
    "iu_mb2_2=sentence_bleu(all_words_iu, mx_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "iu_mb2_3=sentence_bleu(all_words_iu, mx_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_mb2_1)\n",
    "print('Cumulative 2-gram: %f' % iu_mb2_2)\n",
    "print('Cumulative 3-gram: %f' % iu_mb2_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_mb3_1=sentence_bleu(all_words_iu, mx_gen[2], weights=(1, 0, 0, 0))\n",
    "iu_mb3_2=sentence_bleu(all_words_iu, mx_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "iu_mb3_3=sentence_bleu(all_words_iu, mx_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_mb3_1)\n",
    "print('Cumulative 2-gram: %f' % iu_mb3_2)\n",
    "print('Cumulative 3-gram: %f' % iu_mb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "iu_mb4_1=sentence_bleu(all_words_iu, mx_gen[3], weights=(1, 0, 0, 0))\n",
    "iu_mb4_2=sentence_bleu(all_words_iu, mx_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "iu_mb4_3=sentence_bleu(all_words_iu, mx_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_mb4_1)\n",
    "print('Cumulative 2-gram: %f' % iu_mb4_2)\n",
    "print('Cumulative 3-gram: %f' % iu_mb4_3)\n",
    "print('\\n')\n",
    "\n",
    "iu_mb5_1=sentence_bleu(all_words_iu, mx_gen[4], weights=(1, 0, 0, 0))\n",
    "iu_mb5_2=sentence_bleu(all_words_iu, mx_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "iu_mb5_3=sentence_bleu(all_words_iu, mx_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % iu_mb5_1)\n",
    "print('Cumulative 2-gram: %f' % iu_mb5_2)\n",
    "print('Cumulative 3-gram: %f' % iu_mb5_2)\n",
    "print('\\n')\n",
    "avg1=(iu_mb1_1+iu_mb2_1+iu_mb3_1+iu_mb4_1+iu_mb5_1)/5\n",
    "avg2=(iu_mb1_2+iu_mb2_2+iu_mb3_2+iu_mb4_2+iu_mb5_2)/5\n",
    "avg3=(iu_mb1_3+iu_mb2_3+iu_mb3_3+iu_mb4_3+iu_mb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 선우정아 데이터 # CAN = 아이유 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.628743\n",
      "Cumulative 2-gram: 0.319790\n",
      "Cumulative 3-gram: 0.173564\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.469880\n",
      "Cumulative 2-gram: 0.213457\n",
      "Cumulative 3-gram: 0.000000\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.434783\n",
      "Cumulative 2-gram: 0.195047\n",
      "Cumulative 3-gram: 0.063831\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.648045\n",
      "Cumulative 2-gram: 0.346617\n",
      "Cumulative 3-gram: 0.178850\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.553954\n",
      "Cumulative 2-gram: 0.303826\n",
      "Cumulative 3-gram: 0.303826\n",
      "\n",
      "\n",
      "avg 1-gram: 0.547081\n",
      "avg 2-gram: 0.275747\n",
      "avg 3-gram: 0.104925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhz20\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "s_b1_1=sentence_bleu(all_words_s, iu_gen[0], weights=(1, 0, 0, 0))\n",
    "s_b1_2=sentence_bleu(all_words_s, iu_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "s_b1_3=sentence_bleu(all_words_s, iu_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_b1_1)\n",
    "print('Cumulative 2-gram: %f' % s_b1_2)\n",
    "print('Cumulative 3-gram: %f' % s_b1_3)\n",
    "print('\\n')\n",
    "\n",
    "s_b2_1=sentence_bleu(all_words_s, iu_gen[1], weights=(1, 0, 0, 0))\n",
    "s_b2_2=sentence_bleu(all_words_s, iu_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "s_b2_3=sentence_bleu(all_words_s, iu_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_b2_1)\n",
    "print('Cumulative 2-gram: %f' % s_b2_2)\n",
    "print('Cumulative 3-gram: %f' % s_b2_3)\n",
    "print('\\n')\n",
    "\n",
    "s_b3_1=sentence_bleu(all_words_s, iu_gen[2], weights=(1, 0, 0, 0))\n",
    "s_b3_2=sentence_bleu(all_words_s, iu_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "s_b3_3=sentence_bleu(all_words_s, iu_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_b3_1)\n",
    "print('Cumulative 2-gram: %f' % s_b3_2)\n",
    "print('Cumulative 3-gram: %f' % s_b3_3)\n",
    "print('\\n')\n",
    "      \n",
    "s_b4_1=sentence_bleu(all_words_s, iu_gen[3], weights=(1, 0, 0, 0))\n",
    "s_b4_2=sentence_bleu(all_words_s, iu_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "s_b4_3=sentence_bleu(all_words_s, iu_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_b4_1)\n",
    "print('Cumulative 2-gram: %f' % s_b4_2)\n",
    "print('Cumulative 3-gram: %f' % s_b4_3)\n",
    "print('\\n')\n",
    "\n",
    "s_b5_1=sentence_bleu(all_words_s, iu_gen[4], weights=(1, 0, 0, 0))\n",
    "s_b5_2=sentence_bleu(all_words_s, iu_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "s_b5_3=sentence_bleu(all_words_s, iu_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_b5_1)\n",
    "print('Cumulative 2-gram: %f' % s_b5_2)\n",
    "print('Cumulative 3-gram: %f' % s_b5_2)\n",
    "print('\\n')\n",
    "avg1=(s_b1_1+s_b2_1+s_b3_1+s_b4_1+s_b5_1)/5\n",
    "avg2=(s_b1_2+s_b2_2+s_b3_2+s_b4_2+s_b5_2)/5\n",
    "avg3=(s_b1_3+s_b2_3+s_b3_3+s_b4_3+s_b5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 선우정아 데이터 # CAN = 선우정아 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.504808\n",
      "Cumulative 2-gram: 0.283684\n",
      "Cumulative 3-gram: 0.149044\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.704918\n",
      "Cumulative 2-gram: 0.403883\n",
      "Cumulative 3-gram: 0.178928\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.564516\n",
      "Cumulative 2-gram: 0.322101\n",
      "Cumulative 3-gram: 0.168230\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.666667\n",
      "Cumulative 2-gram: 0.410117\n",
      "Cumulative 3-gram: 0.248586\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.310580\n",
      "Cumulative 2-gram: 0.117589\n",
      "Cumulative 3-gram: 0.117589\n",
      "\n",
      "\n",
      "avg 1-gram: 0.550298\n",
      "avg 2-gram: 0.307475\n",
      "avg 3-gram: 0.156446\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "s_sb1_1=sentence_bleu(all_words_s, s_gen[0], weights=(1, 0, 0, 0))\n",
    "s_sb1_2=sentence_bleu(all_words_s, s_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "s_sb1_3=sentence_bleu(all_words_s, s_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_sb1_1)\n",
    "print('Cumulative 2-gram: %f' % s_sb1_2)\n",
    "print('Cumulative 3-gram: %f' % s_sb1_3)\n",
    "print('\\n')\n",
    "\n",
    "s_sb2_1=sentence_bleu(all_words_s, s_gen[1], weights=(1, 0, 0, 0))\n",
    "s_sb2_2=sentence_bleu(all_words_s, s_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "s_sb2_3=sentence_bleu(all_words_s, s_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_sb2_1)\n",
    "print('Cumulative 2-gram: %f' % s_sb2_2)\n",
    "print('Cumulative 3-gram: %f' % s_sb2_3)\n",
    "print('\\n')\n",
    "\n",
    "s_sb3_1=sentence_bleu(all_words_s, s_gen[2], weights=(1, 0, 0, 0))\n",
    "s_sb3_2=sentence_bleu(all_words_s, s_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "s_sb3_3=sentence_bleu(all_words_s, s_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_sb3_1)\n",
    "print('Cumulative 2-gram: %f' % s_sb3_2)\n",
    "print('Cumulative 3-gram: %f' % s_sb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "s_sb4_1=sentence_bleu(all_words_s, s_gen[3], weights=(1, 0, 0, 0))\n",
    "s_sb4_2=sentence_bleu(all_words_s, s_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "s_sb4_3=sentence_bleu(all_words_s, s_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_sb4_1)\n",
    "print('Cumulative 2-gram: %f' % s_sb4_2)\n",
    "print('Cumulative 3-gram: %f' % s_sb4_3)\n",
    "print('\\n')\n",
    "\n",
    "s_sb5_1=sentence_bleu(all_words_s, s_gen[4], weights=(1, 0, 0, 0))\n",
    "s_sb5_2=sentence_bleu(all_words_s, s_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "s_sb5_3=sentence_bleu(all_words_s, s_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_sb5_1)\n",
    "print('Cumulative 2-gram: %f' % s_sb5_2)\n",
    "print('Cumulative 3-gram: %f' % s_sb5_2)\n",
    "print('\\n')\n",
    "avg1=(s_sb1_1+s_sb2_1+s_sb3_1+s_sb4_1+s_sb5_1)/5\n",
    "avg2=(s_sb1_2+s_sb2_2+s_sb3_2+s_sb4_2+s_sb5_2)/5\n",
    "avg3=(s_sb1_3+s_sb2_3+s_sb3_3+s_sb4_3+s_sb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 선우정아 데이터 # CAN = 몬엑 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.319672\n",
      "Cumulative 2-gram: 0.135710\n",
      "Cumulative 3-gram: 0.062852\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.469965\n",
      "Cumulative 2-gram: 0.267696\n",
      "Cumulative 3-gram: 0.159324\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.481100\n",
      "Cumulative 2-gram: 0.290873\n",
      "Cumulative 3-gram: 0.173779\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.414545\n",
      "Cumulative 2-gram: 0.202112\n",
      "Cumulative 3-gram: 0.086385\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.502203\n",
      "Cumulative 2-gram: 0.329977\n",
      "Cumulative 3-gram: 0.329977\n",
      "\n",
      "\n",
      "avg 1-gram: 0.437497\n",
      "avg 2-gram: 0.245274\n",
      "avg 3-gram: 0.138275\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "s_mb1_1=sentence_bleu(all_words_s, mx_gen[0], weights=(1, 0, 0, 0))\n",
    "s_mb1_2=sentence_bleu(all_words_s, mx_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "s_mb1_3=sentence_bleu(all_words_s, mx_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_mb1_1)\n",
    "print('Cumulative 2-gram: %f' % s_mb1_2)\n",
    "print('Cumulative 3-gram: %f' % s_mb1_3)\n",
    "print('\\n')\n",
    "\n",
    "s_mb2_1=sentence_bleu(all_words_s, mx_gen[1], weights=(1, 0, 0, 0))\n",
    "s_mb2_2=sentence_bleu(all_words_s, mx_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "s_mb2_3=sentence_bleu(all_words_s, mx_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_mb2_1)\n",
    "print('Cumulative 2-gram: %f' % s_mb2_2)\n",
    "print('Cumulative 3-gram: %f' % s_mb2_3)\n",
    "print('\\n')\n",
    "\n",
    "s_mb3_1=sentence_bleu(all_words_s, mx_gen[2], weights=(1, 0, 0, 0))\n",
    "s_mb3_2=sentence_bleu(all_words_s, mx_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "s_mb3_3=sentence_bleu(all_words_s, mx_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_mb3_1)\n",
    "print('Cumulative 2-gram: %f' % s_mb3_2)\n",
    "print('Cumulative 3-gram: %f' % s_mb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "s_mb4_1=sentence_bleu(all_words_s, mx_gen[3], weights=(1, 0, 0, 0))\n",
    "s_mb4_2=sentence_bleu(all_words_s, mx_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "s_mb4_3=sentence_bleu(all_words_s, mx_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_mb4_1)\n",
    "print('Cumulative 2-gram: %f' % s_mb4_2)\n",
    "print('Cumulative 3-gram: %f' % s_mb4_3)\n",
    "print('\\n')\n",
    "\n",
    "s_mb5_1=sentence_bleu(all_words_s, mx_gen[4], weights=(1, 0, 0, 0))\n",
    "s_mb5_2=sentence_bleu(all_words_s, mx_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "s_mb5_3=sentence_bleu(all_words_s, mx_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % s_mb5_1)\n",
    "print('Cumulative 2-gram: %f' % s_mb5_2)\n",
    "print('Cumulative 3-gram: %f' % s_mb5_2)\n",
    "print('\\n')\n",
    "avg1=(s_mb1_1+s_mb2_1+s_mb3_1+s_mb4_1+s_mb5_1)/5\n",
    "avg2=(s_mb1_2+s_mb2_2+s_mb3_2+s_mb4_2+s_mb5_2)/5\n",
    "avg3=(s_mb1_3+s_mb2_3+s_mb3_3+s_mb4_3+s_mb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 몬엑 데이터 # CAN = 아이유 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.680425\n",
      "Cumulative 2-gram: 0.342716\n",
      "Cumulative 3-gram: 0.173141\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.538375\n",
      "Cumulative 2-gram: 0.246748\n",
      "Cumulative 3-gram: 0.115878\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.514178\n",
      "Cumulative 2-gram: 0.206905\n",
      "Cumulative 3-gram: 0.082026\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.608939\n",
      "Cumulative 2-gram: 0.374515\n",
      "Cumulative 3-gram: 0.215173\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.653595\n",
      "Cumulative 2-gram: 0.340733\n",
      "Cumulative 3-gram: 0.340733\n",
      "\n",
      "\n",
      "avg 1-gram: 0.606923\n",
      "avg 2-gram: 0.296744\n",
      "avg 3-gram: 0.139630\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "mx_b1_1=sentence_bleu(all_words_mx, iu_gen[0], weights=(1, 0, 0, 0))\n",
    "mx_b1_2=sentence_bleu(all_words_mx, iu_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "mx_b1_3=sentence_bleu(all_words_mx, iu_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_b1_1)\n",
    "print('Cumulative 2-gram: %f' % mx_b1_2)\n",
    "print('Cumulative 3-gram: %f' % mx_b1_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_b2_1=sentence_bleu(all_words_mx, iu_gen[1], weights=(1, 0, 0, 0))\n",
    "mx_b2_2=sentence_bleu(all_words_mx, iu_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "mx_b2_3=sentence_bleu(all_words_mx, iu_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_b2_1)\n",
    "print('Cumulative 2-gram: %f' % mx_b2_2)\n",
    "print('Cumulative 3-gram: %f' % mx_b2_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_b3_1=sentence_bleu(all_words_mx, iu_gen[2], weights=(1, 0, 0, 0))\n",
    "mx_b3_2=sentence_bleu(all_words_mx, iu_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "mx_b3_3=sentence_bleu(all_words_mx, iu_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_b3_1)\n",
    "print('Cumulative 2-gram: %f' % mx_b3_2)\n",
    "print('Cumulative 3-gram: %f' % mx_b3_3)\n",
    "print('\\n')\n",
    "      \n",
    "mx_b4_1=sentence_bleu(all_words_mx, iu_gen[3], weights=(1, 0, 0, 0))\n",
    "mx_b4_2=sentence_bleu(all_words_mx, iu_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "mx_b4_3=sentence_bleu(all_words_mx, iu_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_b4_1)\n",
    "print('Cumulative 2-gram: %f' % mx_b4_2)\n",
    "print('Cumulative 3-gram: %f' % mx_b4_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_b5_1=sentence_bleu(all_words_mx, iu_gen[4], weights=(1, 0, 0, 0))\n",
    "mx_b5_2=sentence_bleu(all_words_mx, iu_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "mx_b5_3=sentence_bleu(all_words_mx, iu_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_b5_1)\n",
    "print('Cumulative 2-gram: %f' % mx_b5_2)\n",
    "print('Cumulative 3-gram: %f' % mx_b5_2)\n",
    "print('\\n')\n",
    "avg1=(mx_b1_1+mx_b2_1+mx_b3_1+s_b4_1+mx_b5_1)/5\n",
    "avg2=(mx_b1_2+mx_b2_2+mx_b3_2+s_b4_2+mx_b5_2)/5\n",
    "avg3=(mx_b1_3+mx_b2_3+mx_b3_3+s_b4_3+mx_b5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 몬엑 데이터 # CAN = 선우정아 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.625000\n",
      "Cumulative 2-gram: 0.384639\n",
      "Cumulative 3-gram: 0.229044\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.713115\n",
      "Cumulative 2-gram: 0.351801\n",
      "Cumulative 3-gram: 0.129947\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.666667\n",
      "Cumulative 2-gram: 0.365148\n",
      "Cumulative 3-gram: 0.174873\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.634703\n",
      "Cumulative 2-gram: 0.421427\n",
      "Cumulative 3-gram: 0.265634\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.290102\n",
      "Cumulative 2-gram: 0.122076\n",
      "Cumulative 3-gram: 0.122076\n",
      "\n",
      "\n",
      "avg 1-gram: 0.585917\n",
      "avg 2-gram: 0.329018\n",
      "avg 3-gram: 0.169548\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "mx_sb1_1=sentence_bleu(all_words_mx, s_gen[0], weights=(1, 0, 0, 0))\n",
    "mx_sb1_2=sentence_bleu(all_words_mx, s_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "mx_sb1_3=sentence_bleu(all_words_mx, s_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_sb1_1)\n",
    "print('Cumulative 2-gram: %f' % mx_sb1_2)\n",
    "print('Cumulative 3-gram: %f' % mx_sb1_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_sb2_1=sentence_bleu(all_words_mx, s_gen[1], weights=(1, 0, 0, 0))\n",
    "mx_sb2_2=sentence_bleu(all_words_mx, s_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "mx_sb2_3=sentence_bleu(all_words_mx, s_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_sb2_1)\n",
    "print('Cumulative 2-gram: %f' % mx_sb2_2)\n",
    "print('Cumulative 3-gram: %f' % mx_sb2_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_sb3_1=sentence_bleu(all_words_mx, s_gen[2], weights=(1, 0, 0, 0))\n",
    "mx_sb3_2=sentence_bleu(all_words_mx, s_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "mx_sb3_3=sentence_bleu(all_words_mx, s_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_sb3_1)\n",
    "print('Cumulative 2-gram: %f' % mx_sb3_2)\n",
    "print('Cumulative 3-gram: %f' % mx_sb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "mx_sb4_1=sentence_bleu(all_words_mx, s_gen[3], weights=(1, 0, 0, 0))\n",
    "mx_sb4_2=sentence_bleu(all_words_mx, s_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "mx_sb4_3=sentence_bleu(all_words_mx, s_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_sb4_1)\n",
    "print('Cumulative 2-gram: %f' % mx_sb4_2)\n",
    "print('Cumulative 3-gram: %f' % mx_sb4_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_sb5_1=sentence_bleu(all_words_mx, s_gen[4], weights=(1, 0, 0, 0))\n",
    "mx_sb5_2=sentence_bleu(all_words_mx, s_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "mx_sb5_3=sentence_bleu(all_words_mx, s_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_sb5_1)\n",
    "print('Cumulative 2-gram: %f' % mx_sb5_2)\n",
    "print('Cumulative 3-gram: %f' % mx_sb5_2)\n",
    "print('\\n')\n",
    "avg1=(mx_sb1_1+mx_sb2_1+mx_sb3_1+mx_sb4_1+mx_sb5_1)/5\n",
    "avg2=(mx_sb1_2+mx_sb2_2+mx_sb3_2+mx_sb4_2+mx_sb5_2)/5\n",
    "avg3=(mx_sb1_3+mx_sb2_3+mx_sb3_3+mx_sb4_3+mx_sb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REF = 몬엑 데이터 # CAN = 몬엑 gen 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.311475\n",
      "Cumulative 2-gram: 0.175394\n",
      "Cumulative 3-gram: 0.088115\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.484099\n",
      "Cumulative 2-gram: 0.318250\n",
      "Cumulative 3-gram: 0.208557\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.563574\n",
      "Cumulative 2-gram: 0.371454\n",
      "Cumulative 3-gram: 0.251618\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.440000\n",
      "Cumulative 2-gram: 0.299878\n",
      "Cumulative 3-gram: 0.207870\n",
      "\n",
      "\n",
      "Cumulative 1-gram: 0.594714\n",
      "Cumulative 2-gram: 0.423013\n",
      "Cumulative 3-gram: 0.423013\n",
      "\n",
      "\n",
      "avg 1-gram: 0.478772\n",
      "avg 2-gram: 0.317598\n",
      "avg 3-gram: 0.213707\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "mx_mb1_1=sentence_bleu(all_words_mx, mx_gen[0], weights=(1, 0, 0, 0))\n",
    "mx_mb1_2=sentence_bleu(all_words_mx, mx_gen[0], weights=(0.5, 0.5, 0, 0))\n",
    "mx_mb1_3=sentence_bleu(all_words_mx, mx_gen[0], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_mb1_1)\n",
    "print('Cumulative 2-gram: %f' % mx_mb1_2)\n",
    "print('Cumulative 3-gram: %f' % mx_mb1_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_mb2_1=sentence_bleu(all_words_mx, mx_gen[1], weights=(1, 0, 0, 0))\n",
    "mx_mb2_2=sentence_bleu(all_words_mx, mx_gen[1], weights=(0.5, 0.5, 0, 0))\n",
    "mx_mb2_3=sentence_bleu(all_words_mx, mx_gen[1], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_mb2_1)\n",
    "print('Cumulative 2-gram: %f' % mx_mb2_2)\n",
    "print('Cumulative 3-gram: %f' % mx_mb2_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_mb3_1=sentence_bleu(all_words_mx, mx_gen[2], weights=(1, 0, 0, 0))\n",
    "mx_mb3_2=sentence_bleu(all_words_mx, mx_gen[2], weights=(0.5, 0.5, 0, 0))\n",
    "mx_mb3_3=sentence_bleu(all_words_mx, mx_gen[2], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_mb3_1)\n",
    "print('Cumulative 2-gram: %f' % mx_mb3_2)\n",
    "print('Cumulative 3-gram: %f' % mx_mb3_3)\n",
    "print('\\n')\n",
    "      \n",
    "mx_mb4_1=sentence_bleu(all_words_mx, mx_gen[3], weights=(1, 0, 0, 0))\n",
    "mx_mb4_2=sentence_bleu(all_words_mx, mx_gen[3], weights=(0.5, 0.5, 0, 0))\n",
    "mx_mb4_3=sentence_bleu(all_words_mx, mx_gen[3], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_mb4_1)\n",
    "print('Cumulative 2-gram: %f' % mx_mb4_2)\n",
    "print('Cumulative 3-gram: %f' % mx_mb4_3)\n",
    "print('\\n')\n",
    "\n",
    "mx_mb5_1=sentence_bleu(all_words_mx, mx_gen[4], weights=(1, 0, 0, 0))\n",
    "mx_mb5_2=sentence_bleu(all_words_mx, mx_gen[4], weights=(0.5, 0.5, 0, 0))\n",
    "mx_mb5_3=sentence_bleu(all_words_mx, mx_gen[4], weights=(0.33, 0.33, 0.33, 0))\n",
    "print('Cumulative 1-gram: %f' % mx_mb5_1)\n",
    "print('Cumulative 2-gram: %f' % mx_mb5_2)\n",
    "print('Cumulative 3-gram: %f' % mx_mb5_2)\n",
    "print('\\n')\n",
    "avg1=(mx_mb1_1+mx_mb2_1+mx_mb3_1+mx_mb4_1+mx_mb5_1)/5\n",
    "avg2=(mx_mb1_2+mx_mb2_2+mx_mb3_2+mx_mb4_2+mx_mb5_2)/5\n",
    "avg3=(mx_mb1_3+mx_mb2_3+mx_mb3_3+mx_mb4_3+mx_mb5_3)/5\n",
    "\n",
    "print('avg 1-gram: %f' % avg1)\n",
    "print('avg 2-gram: %f' % avg2)\n",
    "print('avg 3-gram: %f' % avg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D_n끼리 BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655023284485\n",
      "0.38052148117626816\n",
      "0.18082384940734875\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_mx)):\n",
    "    iu_D_mx_D1=sentence_bleu(all_words_iu, all_words_mx[i], weights=(1, 0, 0, 0))\n",
    "    iu_D_mx_D2=sentence_bleu(all_words_iu, all_words_mx[i], weights=(0.5, 0.5, 0, 0))\n",
    "    iu_D_mx_D3=sentence_bleu(all_words_iu, all_words_mx[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(iu_D_mx_D1)\n",
    "    gram_list2.append(iu_D_mx_D2)\n",
    "    gram_list3.append(iu_D_mx_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6918235454013061\n",
      "0.379201442014279\n",
      "0.1697390054848232\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_s)):\n",
    "    iu_D_s_D1=sentence_bleu(all_words_iu, all_words_s[i], weights=(1, 0, 0, 0))\n",
    "    iu_D_s_D2=sentence_bleu(all_words_iu, all_words_s[i], weights=(0.5, 0.5, 0, 0))\n",
    "    iu_D_s_D3=sentence_bleu(all_words_iu, all_words_s[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(iu_D_s_D1)\n",
    "    gram_list2.append(iu_D_s_D2)\n",
    "    gram_list3.append(iu_D_s_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6579543847865613\n",
      "0.38164236937307144\n",
      "0.18698240876464645\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_mx)):\n",
    "    s_D_mx_D1=sentence_bleu(all_words_s, all_words_mx[i], weights=(1, 0, 0, 0))\n",
    "    s_D_mx_D2=sentence_bleu(all_words_s, all_words_mx[i], weights=(0.5, 0.5, 0, 0))\n",
    "    s_D_mx_D3=sentence_bleu(all_words_s, all_words_mx[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(s_D_mx_D1)\n",
    "    gram_list2.append(s_D_mx_D2)\n",
    "    gram_list3.append(s_D_mx_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6967956409538643\n",
      "0.3761686587605326\n",
      "0.16286227604744172\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_iu)):\n",
    "    s_D_iu_D1=sentence_bleu(all_words_s, all_words_iu[i], weights=(1, 0, 0, 0))\n",
    "    s_D_iu_D2=sentence_bleu(all_words_s, all_words_iu[i], weights=(0.5, 0.5, 0, 0))\n",
    "    s_D_iu_D3=sentence_bleu(all_words_s, all_words_iu[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(s_D_iu_D1)\n",
    "    gram_list2.append(s_D_iu_D2)\n",
    "    gram_list3.append(s_D_iu_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7484780570145819\n",
      "0.4307942909138773\n",
      "0.20376882298042237\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_iu)):\n",
    "    mx_D_iu_D1=sentence_bleu(all_words_mx, all_words_iu[i], weights=(1, 0, 0, 0))\n",
    "    mx_D_iu_D2=sentence_bleu(all_words_mx, all_words_iu[i], weights=(0.5, 0.5, 0, 0))\n",
    "    mx_D_iu_D3=sentence_bleu(all_words_mx, all_words_iu[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(mx_D_iu_D1)\n",
    "    gram_list2.append(mx_D_iu_D2)\n",
    "    gram_list3.append(mx_D_iu_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7380949966500081\n",
      "0.43224172045445247\n",
      "0.22306247084373032\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(all_words_s)):\n",
    "    mx_D_s_D1=sentence_bleu(all_words_mx, all_words_s[i], weights=(1, 0, 0, 0))\n",
    "    mx_D_s_D2=sentence_bleu(all_words_mx, all_words_s[i], weights=(0.5, 0.5, 0, 0))\n",
    "    mx_D_s_D3=sentence_bleu(all_words_mx, all_words_s[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(mx_D_s_D1)\n",
    "    gram_list2.append(mx_D_s_D2)\n",
    "    gram_list3.append(mx_D_s_D3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_n끼리 BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33160776243005635\n",
      "0.12464759281985786\n",
      "0.010005869726404113\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(iu_gen)):\n",
    "    iu_G_mx_G1=sentence_bleu(iu_gen, mx_gen[i], weights=(1, 0, 0, 0))\n",
    "    iu_G_mx_G2=sentence_bleu(iu_gen, mx_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    iu_G_mx_G3=sentence_bleu(iu_gen, mx_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(iu_G_mx_G1)\n",
    "    gram_list2.append(iu_G_mx_G2)\n",
    "    gram_list3.append(iu_G_mx_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4128786634036685\n",
      "0.16093508245423874\n",
      "0.04557442757070945\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(iu_gen)):\n",
    "    iu_G_s_G1=sentence_bleu(iu_gen, s_gen[i], weights=(1, 0, 0, 0))\n",
    "    iu_G_s_G2=sentence_bleu(iu_gen, s_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    iu_G_s_G3=sentence_bleu(iu_gen, s_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(iu_G_s_G1)\n",
    "    gram_list2.append(iu_G_s_G2)\n",
    "    gram_list3.append(iu_G_s_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3805411541989342\n",
      "0.16099198563593073\n",
      "0.07740386299676523\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(mx_gen)):\n",
    "    s_G_mx_G1=sentence_bleu(s_gen, mx_gen[i], weights=(1, 0, 0, 0))\n",
    "    s_G_mx_G2=sentence_bleu(s_gen, mx_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    s_G_mx_G3=sentence_bleu(s_gen, mx_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(s_G_mx_G1)\n",
    "    gram_list2.append(s_G_mx_G2)\n",
    "    gram_list3.append(s_G_mx_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4260672366703859\n",
      "0.1793607178545736\n",
      "0.06457652144895093\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(s_gen)):\n",
    "    s_G_iu_G1=sentence_bleu(s_gen, iu_gen[i], weights=(1, 0, 0, 0))\n",
    "    s_G_iu_G2=sentence_bleu(s_gen, iu_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    s_G_iu_G3=sentence_bleu(s_gen, iu_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(s_G_iu_G1)\n",
    "    gram_list2.append(s_G_iu_G2)\n",
    "    gram_list3.append(s_G_iu_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30796390088605424\n",
      "0.12091401916976732\n",
      "0.011608560277761085\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(iu_gen)):\n",
    "    mx_G_iu_G1=sentence_bleu(mx_gen, iu_gen[i], weights=(1, 0, 0, 0))\n",
    "    mx_G_iu_G2=sentence_bleu(mx_gen, iu_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    mx_G_iu_G3=sentence_bleu(mx_gen, iu_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(mx_G_iu_G1)\n",
    "    gram_list2.append(mx_G_iu_G2)\n",
    "    gram_list3.append(mx_G_iu_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.370891536419606\n",
      "0.16336952398834642\n",
      "0.08195517775643393\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "gram_list1=[]\n",
    "gram_list2=[]\n",
    "gram_list3=[]\n",
    "\n",
    "for i in range(len(iu_gen)):\n",
    "    mx_G_s_G1=sentence_bleu(mx_gen, s_gen[i], weights=(1, 0, 0, 0))\n",
    "    mx_G_s_G2=sentence_bleu(mx_gen, s_gen[i], weights=(0.5, 0.5, 0, 0))\n",
    "    mx_G_s_G3=sentence_bleu(mx_gen, s_gen[i], weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_list1.append(mx_G_s_G1)\n",
    "    gram_list2.append(mx_G_s_G2)\n",
    "    gram_list3.append(mx_G_s_G3)\n",
    "\n",
    "print(statistics.mean(gram_list1))\n",
    "print(statistics.mean(gram_list2))\n",
    "print(statistics.mean(gram_list3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
